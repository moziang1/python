## 浏览器爬虫伪装



### 服务器检测是检测请求包的header中的User-Agent

我们想要绕过服务器检测就需要修改这个

有两种方法

### 方法一，直接修改请求时的头部信息（User-Agent是以字典的形式存储的）

```python
import urllib.request
import urllib.parse
import json

while True:
    haha = input('输入你想要翻译的单词或语句：')
    # 这里先生成一个和User-Agent一样的字典（直接调用正常访问的就行）我这里是直接复制网页的
    
    
        # 添加头部信息反爬取
    head = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64)\
    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.3964.2 Safari/537.36"}

    res = requests.get(url, headers=header)
    # 上面这个是翻译网站时浏览器所持有的身份验证User-Agent，我们替换的就是这个，如果不替换，默认是pythonxxx版本，容易被拦截

    url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
    date = {}
    date['i'] = haha
    date['from'] = 'AUTO'
    date['to'] = 'AUTO'
    date['smartresult'] = 'dict'
    date['client'] = 'fanyideskweb'
    date['salt'] = '15755234720405'
    date['sign'] = '393a0a7bc1d6619962c71ae755574915'
    date['ts'] = '1575523472040'
    date['bv'] = 'bc250de095a39eeec212da07435b6924'
    date['doctype'] = 'json'
    date['version'] = '2.1'
    date['keyfrom'] = 'fanyi.web'
    date['action'] = 'FY_BY_CLICKBUTTION'

    # data参数如果要传必须传bytes（字节流）类型的，如果是一个字典，先用urllib.parse.urlencode()编码。
    date = urllib.parse.urlencode(date).encode('utf-8')

    # 在这里要再添加一个head，修改上传时的请求头部信息
    # headers不能通过urllib.request.urlopen()发送，要通过urllib.request.Request()发送
    request = urllib.request.Request(url=url, data=date, headers=head)
    response = urllib.request.urlopen(request)

    # 响应完的数据是一个json值，我们先转换为utf-8的形式
    htmlPage = response.read().decode('utf-8')
    # 将htmlPage重构成普通数据，他是一个字典
    target = json.loads(htmlPage)
    print(target['translateResult'][0][0]["tgt"])

    # 验证头部信息是否被更改
    #print(request.headers)
```



### 方法二   再请求头部生成后再添加

```python
import urllib.request
import urllib.parse
import json

while True:
    haha = input('输入你想要翻译的单词或语句：')
    url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
    date = {}
    date['i'] = haha
    date['from'] = 'AUTO'
    date['to'] = 'AUTO'
    date['smartresult'] = 'dict'
    date['client'] = 'fanyideskweb'
    date['salt'] = '15755234720405'
    date['sign'] = '393a0a7bc1d6619962c71ae755574915'
    date['ts'] = '1575523472040'
    date['bv'] = 'bc250de095a39eeec212da07435b6924'
    date['doctype'] = 'json'
    date['version'] = '2.1'
    date['keyfrom'] = 'fanyi.web'
    date['action'] = 'FY_BY_CLICKBUTTION'

    date = urllib.parse.urlencode(date).encode('utf-8')

    request = urllib.request.Request(url=url, data=date)
    # 直接再这里添加头部信息UserAgent-value（request.add_header）
    request.add_header('UserAgent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36')

    response = urllib.request.urlopen(request)

    htmlPage = response.read().decode('utf-8')
    target = json.loads(htmlPage)
    print(target['translateResult'][0][0]["tgt"])

    #验证头部信息是否被更改
    #print(request.headers)
```

小技巧

```python
#这个是作为备用的headers头部使用(浏览器伪装)
    my_headers=["Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36", 
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36", 
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0"
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/537.75.14", 
"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Win64; x64; Trident/6.0)"]
    
    #每次访问随机给一个headers头部，减少浏览器403的错误
    randdom_header=random.choice(headers) 
    #因为上面创建的是一个列表，在这里去随机浏览器列表

    
    req = urllib.request.Request(url)
    
    #添加为伪装头部
    req.add_header("User-Agent",randdom_header) 
  	req.add_header("Host","www.mzitu.com") 
  	req.add_header("Referer","https://www.mzitu.com/") 
 	req.add_header("GET",url) 
    
    #在open之前添加
    response = urllib.request.urlopen(req)
```















### headers是解决了python代码访问的弊端，但是如果我们是爬取图片的时候那

同一个ip突然间有巨大的流量下载，不就告诉服务器我是非人类吗？

服务器屏蔽的爬虫的规则，检查ip的访问



#### 我们反针对的方法主要有两种

```
1.延迟提交时间

2.代理
```

### 1.延迟提交时间

```python
import urllib.request
import urllib.parse
import json
import time  # 添加时间模块

while True:
    haha = input('输入你想要翻译的单词，输入（Q/q）退出程序：')
    if haha == 'q' or haha == 'Q':  # 当输入q时退出脚本
        break
    url = 'http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule'
    date = {}
    date['i'] = haha
    date['from'] = 'AUTO'
    date['to'] = 'AUTO'
    date['smartresult'] = 'dict'
    date['client'] = 'fanyideskweb'
    date['salt'] = '15755234720405'
    date['sign'] = '393a0a7bc1d6619962c71ae755574915'
    date['ts'] = '1575523472040'
    date['bv'] = 'bc250de095a39eeec212da07435b6924'
    date['doctype'] = 'json'
    date['version'] = '2.1'
    date['keyfrom'] = 'fanyi.web'
    date['action'] = 'FY_BY_CLICKBUTTION'

    date = urllib.parse.urlencode(date).encode('utf-8')

    request = urllib.request.Request(url=url, data=date)

    request.add_header('UserAgent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36')

    response = urllib.request.urlopen(request)

    htmlPage = response.read().decode('utf-8')
    target = json.loads(htmlPage)
    print(target['translateResult'][0][0]["tgt"])

    # 延迟器(当上面的语句执行到这里时，延迟5s再进入循环)
    time.sleep(5)
```



## 2. 使用代理(不同的ip去访问服务器)

步骤:

设置一个字典

1.参数是一个字典{'类型','代理ip:端口号'}

```python
proxy_support = urllib.request.ProxyHandler({})
```

2.定制、创建一个opener

```
opener = urllib.request.build_opener(proxy_support)
```

3 安装 opener  (将opener安装到系统中，一劳永逸，以后只要使用opener函数，就会调用安装的opener的函数（代理）)

```
urllib.request.install_opener()
```

3.x   临时使用opener

```
opener.open(url)
```



实例

首选需要找一个代理ip（我是随便找的）

```python
import urllib.request

# 这个网站，是测试你目前的主机ip是什么
url = 'https://www.whatismyip.com.tw/'

# 开始步骤
proxy_support = urllib.request.ProxyHandler({'http': '114.239.149.191:808'})
# 创建opener
opener = urllib.request.build_opener(proxy_support)
# 安装opener
urllib.request.install_opener(opener)

#如果不加上下面的这行出现会出现urllib2.HTTPError: HTTP Error 403: Forbidden错误
#主要是由于该网站禁止爬虫导致的，可以在请求加上头信息，伪装成浏览器访问User-Agent,具体的信息可以通过火狐的FireBug插件查询
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'}
req = urllib.request.Request(url=url, headers=headers)
html = urllib.request.urlopen(req).read()
print(html)


```



#几种报错情况

```
#10060 代理地址有问题

#403  网站反爬
```



### 获取代理ip地址 py

```python
import re
import requests

url='https://www.kuaidaili.com/free/'   #网络地址

# 定义获取网页
def get_html(url):
    header = {}
    header['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; WOW64) ' \
                           'AppleWebKit/537.36 (KHTML, like Gecko) ' \
                           'Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0'
    proxy={
        'http': '60.220.200.152',
        'http': '115.218.209.109',
    }
    resposed = requests.get(url,headers=header,proxies=proxy)
    html = resposed.text  # 返回页面
    return html

# 定义获取ip代理
def get_ipport(html):
    regex1 = r'<td data-title="IP">(.+)</td>'  # IP
    IP = re.findall(regex1, html)

    regex2 = r'<td data-title="PORT">(.+)</td>'  # 端口号
    PORT = re.findall(regex2, html)

    regex3 = r'<td data-title="类型">(.+)</td>'  # 类型
    TypeList = re.findall(regex3, html)

    summary = []  # 列表
    c = zip(TypeList, IP, PORT, )
    for num in c:
        num = list(num)  # 将元组转化为列表
        num = num[0] + '://' + num[1] + ':' + num[2]  # 整理列表使其美观
        summary.append(num)  # 将其加入列表
    print('高匿代理')
    print(summary)
    return summary

if __name__=='__main__':
    html=get_html(url)
    get_ipport(html)
```



还有另一个

```

import urllib.request
import re

Error = False

class Ip:
    ip = ''
    dk = 80  #默认80
    lx = 'HTTP'

def open_url(url):
    try:
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36')
        page = urllib.request.urlopen(req)
        html = page.read().decode('utf-8')
    except urllib.error.HTTPError as e:
        print('发生错误了:',e)
        html = ''
        Errror = True
        return html
    except urllib.error.URLError as e:
        print('发生错误了:',e)
        html = ''
        Error = True
        return html
    else:
        return html

def get_ip(html):
    if Error == False:
        dk = []
        lx = []
        p = r'(?:(?:[0,1]?\d?\d|2[0-4]\d|25[0-5])\.){3}(?:[0,1]?\d?\d|2[0-4]\d|25[0-5])'
        a = html.find('<td data-title="PORT">')
        b = html.find('</td>',a)
        dk.append(html[a+22:b])
        a = html.find('<td data-title="类型">')
        b = html.find('</td>',a)
        lx.append(html[a+20:b])
        iplist = re.findall(p, html)
        for each in iplist:
            for each_dk in dk:
                for each_lx in lx:
                    ip = Ip()
                    ip.ip = each
                    ip.dk = each_dk
                    ip.lx = each_lx
                    return ip
    else:
        return None

    
if __name__ == '__main__':
    i = 1
    while i<1000:
        url = "http://www.kuaidaili.com/free/inha/"+str(i)
        ipaddrs = Ip()
        ipaddrs = get_ip(open_url(url))
        if Error == False and ipaddrs != None:
            print('IP:'+ipaddrs.ip+'  端口:',str(ipaddrs.dk)+'   协议:'+ipaddrs.lx)
            i+=1
        else:
            Error = False  #发生错误后自恢复


```



